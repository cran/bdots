\documentclass[12pt]{article}
\usepackage{amsmath}
\usepackage{Sweave}
\usepackage{authblk}
\usepackage{fullpage}

\newcommand{\pkg}[1]{{\normalfont\fontseries{b}\selectfont #1}}
\let\proglang=\textsf

\title{Eyetracking Analysis in \proglang{R}}
\author{Michael Seedorff\\Department of Biostatistics\\University of Iowa
   \and Jacob Oleson\\Department of Biostatistics\\University of Iowa
   \and Grant Brown\\Department of Biostatistics\\University of Iowa
   \and Joseph Cavanaugh\\Department of Biostatistics\\University of Iowa
   \and Bob McMurray\\Departments of Psychology, Communication Sciences and Disorders, and Linguistics\\University of Iowa}

\date{}
\begin{document}
\maketitle

\begin{abstract}
The package \pkg{bdots} provides techniques for analyzing eyetracking data,
as well as other types of highly correlated data that consist of many consecutive
tests. We explain how to set the data up in a fashion for the package to use,
how to analyze the data using the built in functions, and how to check the fit
to the data.
\end{abstract}

%\VignetteIndexEntry{Eyetracking Analysis in R}
%\VignetteDepends{bdots}
%\VignetteKeywords{ts}
%\VignettePackage{bdots}

<<echo=FALSE>>=
options(prompt = "R> ", continue = "+  ")
library("bdots")
@

\newpage
\section{Introduction}

If using \pkg{bdots} for analysis in a paper, please cite current version using:

<<eval=FALSE>>=
citation("bdots")
@

For a reference to the statistical methods used in this package, see \cite{eyetracking}.
In this vignette we provide a sample analysis
of eyetracking data obtained from the study \cite{farris2014process}. We use the following
2 functions to fit the data: a 4-parameter logistic and an asymmetric Gaussian. The
4-parameter logistic is defined as the following:

\[
p_{it} = B_i + \frac{P_i - B_i}{1 + \exp \left( 4 * S_i \frac{(C_i - t)}{(P_i - B_i)} \right)}
\]

<<fig=TRUE,echo=FALSE>>=
mini <- 0
peak <- 1
slope <- 0.002
cross <- 800

f <- function(time, mini, peak, slope, cross)
	mini + (peak - mini) / (1 + exp(4 * slope * (cross - (time)) / (peak - mini)))

plot(function(x) f(x, mini, peak, slope, cross), 0, 2000, xlab = "Time (ms)", ylab = "Proportion of Fixations")
lines(c(0, 500), c(0, 0), lty = 2)
lines(c(1000, 2000), c(1, 1), lty = 2)
points(800, 0.5, pch = 16)
lines(c(800, 800), c(0.2, 0.49), lty = 2)
lines(c(600, 1000), c(0.5 - 200 * slope, 0.5 + 200 * slope), lty = 2)
text(c(575, 925, 800, 1000), c(0, 1, 0.18, 0.92), c("B", "P", "C", "S"))
@

\newpage
The asymmetric Gaussian is defined as the following:

\[
p_{it} = \left\{\begin{matrix}
(H_i - B_{1i}) * \exp \left(\frac{(t-\mu_i)^2}{-2\sigma_{1i}^2} \right ) + B_{1i} & \text{if } t \le \mu_i \\ 
(H_i - B_{2i}) * \exp \left(\frac{(t-\mu_i)^2}{-2\sigma_{2i}^2} \right ) + B_{2i} & \text{if } t > \mu_i
\end{matrix}\right.
\]

<<fig=TRUE,echo=FALSE>>=
mu <- 800
ht <- 0.2
base1 <- 0
base2 <- 0.05
sig1 <- 175
sig2 <- 300

f <- function(time, mu, ht, base1, base2, sig1, sig2)
	(time < mu) * (exp(-1 * (time - mu) ^ 2 / (2 * sig1 ^ 2)) * (ht - base1) + base1) +
		(mu <= time) * (exp(-1 * (time - mu) ^ 2 / (2 * sig2 ^ 2)) * (ht - base2) + base2)

plot(function(x) f(x, mu, ht, base1, base2, sig1, sig2), 0, 2000, xlab = "Time (ms)", ylab = "Proportion of Fixations",
	xaxt = "n")
axis(1, at = c(0, 500, 800, 1000, 1500, 2000), labels = TRUE)
lines(c(0, 500), c(0, 0), lty = 2)
lines(c(1200, 2000), c(0.05, 0.05), lty = 2)
lines(c(500, 1100), c(0.20, 0.20), lty = 2)
lines(c(800, 800), c(0.02, 0.20), lty = 2)
lines(c(800, 800), c(-0.01, 0.01), lty = 2)
arrows(800, f(800 + 300 * 1.5, mu, ht, base1, base2, sig1, sig2),
	800 + 300 * 1.5, f(800 + 300 * 1.5, mu, ht, base1, base2, sig1, sig2),
	lty = 2, length = 0.12, code = 3)
arrows(800, f(800 - 175 * 1.5, mu, ht, base1, base2, sig1, sig2),
	800 - 175 * 1.5, f(800 - 175 * 1.5, mu, ht, base1, base2, sig1, sig2),
	lty = 2, length = 0.12, code = 3)
#text(c(625, 1075, 675, 1000, 800, 1200), c(0, 0.05, 0.07, 0.11, 0.015, 0.2),
#	c("Base 1", "Base 2", "Sig 1", "Sig 2", "Mu", "Height"))

text(c(560, 1150, 675, 1000, 800, 1150), c(0, 0.05, 0.07, 0.105, 0.014, 0.2),
	c(expression(B[1]), expression(B[2]),
		expression(sigma[1]), expression(sigma[2]),
		expression(mu), "H"))
@

While only these 2 functions have been implemented, it would be straightforward to
implement more functions in the future (e.g. polynomials).

\newpage
\section{Curve Fitting}
\subsection{Formatting Data}
The data needs the following columns:
\begin{itemize}
	\item Time: A column labelled `Time' that tracks the time variable (the `x' values)
	\item Group: A column labelled `Group' that designates the 2 groups
		that will be compared (should only have 2 unique values in the column)
	\item Subject: A column named `Subject' that uses numbers to designate each
		of the individual curves within a group
	\item Data: A column containing the observed value at each time point (the `y' values)
\end{itemize}


<<>>=
data(ci)
names(ci)[1] <- "Group"
head(ci)
@

\subsection{Fitting Individual Curves}

The first step is fitting each individual's curve to the specified function. There
are 2 implemented functions: the 4-parameter logistic and the asymmetric Gaussian
(referred to as a Double Gaussian in the code). For the first example, we use
the logistic to compare the looks to a `target' object in an eyetracking experiment
between a group of normal hearing individuals and a group of individuals with
cochlear implants (a device used to restore hearing to deaf or near-deaf
individuals).

<<>>=
ci.1 <- subset(ci, ci$LookType == "Target")
@

We want to fit the logistic to each of the individual curves. For this purpose,
the fitting method requires an initial estimate as to the autocorrelation between
the errors (generally alright to leave this at the default of 0.9). Using this
correlation between the errors allows for better estimates of the standard
deviations around the mean estimates, which will be used later in the bootstrapping
section (correlation along the errors can be turned off for
curvefitting by setting \texttt{cor = FALSE}). If a curve cannot be fit using
AR1 correlation assumption, the curve is fit without the autocorrelation. If a curve
can be fit with the AR1 correlation assumption but is a poor fit to the observed curve,
the user should manually refit the curve using approaches discussed in section
\ref{sec:refit}. The status of each individual's AR1 vs non-AR1 fit can be found
by looking in the \texttt{coef.id\#} output vector. \texttt{coef.id1} corresponds
to the first curves for the first group. \texttt{coef.id2} corresponds to the first
curves for the second group. \texttt{coef.id3} and \texttt{coef.id4} correspond
to the second curves for the first and second groups, if \texttt{diffs = TRUE}.


The designation of more than 1 core for processing can significantly reduce time
required for computation. A general recommendation is to set the number of cores
used equal to the total number of cores available (only true physical cores)
minus 1. For instance, we are running this on an Intel Core i7 2600 with 4 physical
cores in addition to 4 `hyper-threaded cores'. We would use 3 cores for the
computations (4 physical cores - 1). For compilation of package vignettes
CRAN has a limit of 2 cores being used in parallel so we will limit ourselves to
2 for this vignette.

Finally, the column corresponding to the observed values needs to be designated
as a number.

<<>>=
ci.1.out.1 <- logistic.fit(ci.1, col = 4, rho.0 = 0.9, cor = TRUE,
	cores = 2)

ci.1.out.1$cor.1
ci.1.out.1$cor.2
@

For our second example we are going to look at the difference in a person's looks
to `cohort' and `unrelated' objects. We then want to compare these between the
normal hearing and cochlear implant groups. We maintain the same structure of the
data, but use an additional column called `Curve' using the numbers 1 and 2 to
designate whether that curve corresponds to a `cohort' or `unrelated' eyetrack.
Additionally, the parameter \texttt{diffs} should be set to TRUE, in this situation.

For this example we will set the cores equal to 1 to show some extra output you
receive by setting this option.

<<>>=
ci.2 <- subset(ci, ci$LookType == "Cohort" | ci$LookType == "Unrelated")
ci.2$Curve <- ifelse(ci.2$LookType == "Cohort", 1, 2)
ci.2.out.1 <- doubleGauss.fit(ci.2, col = 4, rho.0 = 0.9, cor = TRUE, 
	cores = 1, diffs = TRUE)
@

\newpage

\subsection{Checking Curve Fits}

After fitting all the curves, the quality of these fits should be checked. There
are a few ways to check this. The two most common are plotting the histograms for
for each of the estimated parameter values (\texttt{ests.plot}) and plotting
the observed curve and the estimated curve for each subject (\texttt{subs.plot}).

<<eval=FALSE>>=
subs.plot(ci.1.out.1, "topleft")
@

<<fig=TRUE>>=
ests.plot(ci.1.out.1)
@

Looking at the second example, there are some curves that need attending to.
There are too many plots to show in this vignette, but you can view these on your
local machine by running the example code.

<<eval=FALSE>>=
subs.plot(ci.2.out.1)
@

<<fig=TRUE>>=
ests.plot(ci.2.out.1)
@

\subsection{Refitting Bad Curves} \label{sec:refit}

As seen in example 2 (ci.2.*), sometimes the observed curves just aren't fit very well. In
these examples, there are 2 approaches to take. The first: designate better starting
parameters for the curve fit. When initial parameter estimates are too far from a
reasonable curve fit, the curve fitter can't find a good fit. Coming up with new initial
parameter estimates is typically done by looking at an individual curve and making a visual
estimation as to the parameters (a bit more tougher to do with the double gauss). While the first
approach typically failes to fix a bad fit, the second approach will be successful more often:
relax the autocorrelation assumption of the errors. However,
it will increase standard error of the estimates and reduces overall power. First
we'll try refitting with given parameter estimates. For the first curve (subject 13,
group 2, curve 2) we will use the following initial parameter estimates: $\mu=650$,
$H=0.15$, $\sigma_1^2=150$, $\sigma_2^2=100$, $B_1=0$, and $B_2=0.03$. For the
second curve (subject 23, group 2, curve 2) we will use the following initial
parameter estimates: $\mu=700$, $H=0.10$, $\sigma_1^2=150$, $\sigma_2^2=100$,
$B_1=0$, and $B_2=0.01$.

<<>>=
ci.2.out.1 <- doubleGauss.refit(ci.2.out.1 ,
	subj = c(13, 23),
	group = c(2, 2),
	curves = c(2, 2),
	params = list(c(650, 0.15, 150, 100, 0, 0.03),
		c(700, 0.10, 150, 100, 0, 0.01)))
@

This didn't help the estimates at all. We'll now use the second approach, relaxing
the error correlation assumption (\texttt{cor = FALSE}).

<<>>=
ci.2.out.1 <- doubleGauss.refit(ci.2.out.1 ,
	subj = c(13, 23),
	group = c(2, 2),
	curves = c(2, 2),
	cor = c(FALSE, FALSE))
@

\newpage
\section{Bootstrapping and Analysis}

\subsection{Bootstrapping}

The final step is to bootstrap the group mean curve. At each bootstrap curve,
every individual's curve is fit using their estimated parameters and standard
deviation. These are then averaged across each group to get the bootstrap
average. The following are the argument inputs:
\begin{itemize}
	\item seed: the random number seed for reproducible results. By default, this
		is set based on the time at running the method
	\item alpha: the family wise overall P(TIE)
	\item paired: whether the subjects in both groupings are the same and paired
		t-tests should be used
	\item N.iter: the number of bootstrap iterations
	\item cores: the number of cores to use. Typically should be set to the number
		of available true physical cores minus 1
	\item p.adj: `oleson' is our Bonferroni adjustment based on the correlation of
		the test statistics. FDR is another option that can be used by specifying `fdr'
	\item test.spots: the time values to test at using the adjusted p-value. Our
		sample data is observed at every 4ms (from 0 to 2000). If we wanted to
		test at every 8ms we would set this to \texttt{seq(0, 2000, by = 8)}
	\item time.test: the individual time values to test at using an unadjusted
		p-value. These are typically only a couple individual spots of particular
		interest
	\item test.params: whether to test for differences in mean parameter estimates
		between the groups. This is a t-test using the bootstrapped means. If
		\texttt{paired = FALSE}, performs a 2-sample t-test with the equal variance
		assumption. If \texttt{paired = TRUE}, performs a paired t-test. This should
		only be set to \texttt{TRUE} when \texttt{diffs = FALSE}, or the results won't
		make much sense (tests for differences between group 1 curve 1 and group 2 curve 1).
\end{itemize}

Output includes the adjusted alpha value, the significant regions on the time course,
and a graph with the bootstrapped group averages and areas of significance highlighted.

<<fig=TRUE>>=
ci.1.out.2 <- logistic.boot(ci.1.out.1, seed = 123, cores = 2)
@

We will run this again (suppressing the output plot, which will look the same),
but testing for difference in parameter estimates between the groups.

<<fig=FALSE>>=
logistic.boot(ci.1.out.1, seed = 123, cores = 2, test.params = TRUE)
@

For the next example we will also run a non-adjusted test at the time point 900.

<<fig=TRUE>>=
ci.2.out.2 <- doubleGauss.boot(ci.2.out.1, seed = 123, cores = 2, time.test = 900)
@

\newpage
\subsection{Replotting}

There are no options for modifying the plotting properties in the bootstrap step,
as we wanted those options to be focused on the computational issues. Typically
we will want to adjust the size of the y-axis and, by extension, the size of the
highlighting box in terms of the y-axis values.

<<fig=TRUE>>=
replot(ci.1.out.2, bucket.lim = c(0, 1), main = "Example 1 Curve")
@

\newpage
<<fig=TRUE>>=
replot(ci.2.out.2, ylim = c(-0.01, 0.1), bucket.lim = c(0, 0.08),
	main = "Example 2 Curve")
@

\newpage
\bibliographystyle{plain}
\bibliography{bdots}

\end{document}
